{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAM++ Speaker Verification Deep Tutorial\n",
        "# CAM++ 声纹识别深度教程\n",
        "\n",
        "**Author**: 3D-Speaker Team  \n",
        "**Target Audience**: Deep learning beginners, speaker recognition researchers  \n",
        "**Estimated Time**: 4-6 hours\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Fundamentals](#1-fundamentals)\n",
        "   - Speaker Verification Introduction\n",
        "   - Common Features (MFCC, Fbank)\n",
        "   - Loss Functions (ArcFace, AAM Loss)\n",
        "2. [CAM++ Architecture Deep Dive](#2-cam-architecture-deep-dive)\n",
        "   - ResNet Backbone\n",
        "   - Multi-scale Pooling\n",
        "   - Context-Aware Design\n",
        "3. [Implementation](#3-implementation)\n",
        "   - Data Preprocessing\n",
        "   - Model Definition\n",
        "   - Training Loop and Validation\n",
        "4. [Model Export and Optimization](#4-model-export-and-optimization)\n",
        "   - ONNX Conversion\n",
        "   - INT8 Quantization\n",
        "5. [Embedded Deployment Guide](#5-embedded-deployment-guide)\n",
        "   - RK3568 Development Board\n",
        "   - Real-time Inference Script\n",
        "6. [Extensions](#6-extensions)\n",
        "   - Industrial Anomaly Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 1. Fundamentals\n",
        "\n",
        "## 1.1 Speaker Verification Introduction\n",
        "\n",
        "**Speaker Verification/Recognition** is a biometric identification technology based on speech signals that confirms or identifies a speaker's identity by analyzing their acoustic characteristics.\n",
        "\n",
        "### Main Task Types\n",
        "\n",
        "| Task Type | Description | Application |\n",
        "|-----------|-------------|-------------|\n",
        "| **Speaker Verification** | 1:1 verification | Identity authentication, security systems |\n",
        "| **Speaker Identification** | 1:N identification | Meeting transcription, smart assistants |\n",
        "| **Speaker Diarization** | Segment audio by speaker | Conference transcription, call analysis |\n",
        "\n",
        "### Technical Pipeline\n",
        "\n",
        "```\n",
        "Audio Input -> Preprocessing -> Feature Extraction -> Speaker Embedding -> Similarity -> Decision\n",
        "     |              |                |                      |                |           |\n",
        "     +-- Denoise ---+--- Fbank ------+------ CAM++ ---------+--- Cosine -----+-- Threshold\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Environment Check and Dependencies\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add project root to path\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(f\"Project Root: {project_root}\")\n",
        "\n",
        "# Check required dependencies\n",
        "required_packages = ['torch', 'torchaudio', 'numpy', 'matplotlib']\n",
        "missing_packages = []\n",
        "\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"[OK] {package} installed\")\n",
        "    except ImportError:\n",
        "        missing_packages.append(package)\n",
        "        print(f\"[X] {package} not installed\")\n",
        "\n",
        "if missing_packages:\n",
        "    print(f\"\\nPlease install missing packages: pip install {' '.join(missing_packages)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Common Acoustic Features\n",
        "\n",
        "### 1.2.1 MFCC (Mel-Frequency Cepstral Coefficients)\n",
        "\n",
        "MFCC is the classic speech feature that simulates human auditory perception:\n",
        "\n",
        "**Extraction Pipeline**:\n",
        "1. Pre-emphasis\n",
        "2. Framing & Windowing\n",
        "3. FFT (Fast Fourier Transform)\n",
        "4. Mel Filter Bank\n",
        "5. Log operation\n",
        "6. DCT (Discrete Cosine Transform)\n",
        "\n",
        "### 1.2.2 Fbank (Filter Bank Features)\n",
        "\n",
        "Fbank is the predecessor of MFCC, preserving more spectral information:\n",
        "\n",
        "**Advantages**:\n",
        "- Preserves more raw spectral information\n",
        "- Suitable for deep learning models\n",
        "- High computational efficiency\n",
        "\n",
        "**This project uses**: 80-dimensional Fbank features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Feature Extraction Demonstration\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulate audio signal (use real audio in practice)\n",
        "sample_rate = 16000\n",
        "duration = 3.0  # seconds\n",
        "t = np.linspace(0, duration, int(sample_rate * duration))\n",
        "\n",
        "# Generate simulated speech signal (sum of sinusoids)\n",
        "frequencies = [200, 400, 800, 1200]  # fundamental and harmonics\n",
        "signal = np.zeros_like(t)\n",
        "for i, f in enumerate(frequencies):\n",
        "    signal += (1.0 / (i + 1)) * np.sin(2 * np.pi * f * t)\n",
        "signal = signal / np.max(np.abs(signal))  # normalize\n",
        "\n",
        "print(f\"Sample Rate: {sample_rate} Hz\")\n",
        "print(f\"Duration: {duration} s\")\n",
        "print(f\"Number of Samples: {len(signal)}\")\n",
        "\n",
        "# Visualize waveform\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(t[:1600], signal[:1600])  # show first 100ms\n",
        "plt.title('Audio Waveform (First 100ms)')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Simple spectrogram\n",
        "try:\n",
        "    from scipy import signal as scipy_signal\n",
        "    frequencies_spec, times_spec, Sxx = scipy_signal.spectrogram(\n",
        "        signal, sample_rate, nperseg=400, noverlap=240)\n",
        "    plt.pcolormesh(times_spec, frequencies_spec[:50], \n",
        "                   10 * np.log10(Sxx[:50] + 1e-10))\n",
        "    plt.title('Spectrogram')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "    plt.colorbar(label='Power (dB)')\n",
        "except ImportError:\n",
        "    plt.text(0.5, 0.5, 'scipy not installed', ha='center', va='center')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Fbank Feature Extraction (using project's FBank class)\n",
        "try:\n",
        "    from speakerlab.process.processor import FBank\n",
        "    \n",
        "    # Create Fbank feature extractor\n",
        "    fbank_extractor = FBank(\n",
        "        n_mels=80,           # Number of Mel filters\n",
        "        sample_rate=16000,   # Sample rate\n",
        "        mean_nor=True        # Mean normalization\n",
        "    )\n",
        "    \n",
        "    # Convert to tensor and extract features\n",
        "    wav_tensor = torch.from_numpy(signal).float()\n",
        "    fbank_features = fbank_extractor(wav_tensor)\n",
        "    \n",
        "    print(f\"Fbank Feature Shape: {fbank_features.shape}\")\n",
        "    print(f\"Time Frames: {fbank_features.shape[0]}\")\n",
        "    print(f\"Mel Bins: {fbank_features.shape[1]}\")\n",
        "    \n",
        "    # Visualize Fbank features\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.imshow(fbank_features.T.numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
        "    plt.colorbar(label='Magnitude')\n",
        "    plt.title('Fbank Features (80-dim)')\n",
        "    plt.xlabel('Time Frame')\n",
        "    plt.ylabel('Mel Frequency Bin')\n",
        "    plt.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Feature extraction demo requires complete project environment: {e}\")\n",
        "    print(\"Please run this notebook from the project root directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Loss Functions\n",
        "\n",
        "### 1.3.1 Softmax Loss (Baseline)\n",
        "\n",
        "Standard cross-entropy loss for classification.\n",
        "\n",
        "**Drawback**: Intra-class distance may be large, inter-class distance may be small\n",
        "\n",
        "### 1.3.2 ArcFace Loss (Additive Angular Margin Loss)\n",
        "\n",
        "Adds margin in angular space to enhance inter-class separability.\n",
        "\n",
        "Key parameters:\n",
        "- cosine: Cosine similarity between feature and class weight\n",
        "- sine: Computed from cosine (sine of the angle)\n",
        "- phi: cos(theta + m), used for margin penalty\n",
        "\n",
        "### 1.3.3 Implementation in this Project\n",
        "\n",
        "See [`speakerlab/loss/margin_loss.py`](../speakerlab/loss/margin_loss.py) for the full implementation.\n",
        "\n",
        "You can import it in your code:\n",
        "```python\n",
        "from speakerlab.loss.margin_loss import ArcMarginLoss\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ArcFace Loss Implementation Analysis\n",
        "# Simplified demo based on speakerlab/loss/margin_loss.py (see source for full implementation)\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ArcMarginLossDemo(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement Additive Angular Margin Loss (ArcFace)\n",
        "    \n",
        "    Args:\n",
        "        scale: Scale factor, controls decision boundary temperature\n",
        "        margin: Angular margin, increases positive sample difficulty\n",
        "        easy_margin: Whether to use simplified margin\n",
        "    \"\"\"\n",
        "    def __init__(self, scale=32.0, margin=0.2, easy_margin=False):\n",
        "        super(ArcMarginLossDemo, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.easy_margin = easy_margin\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.update(margin)\n",
        "\n",
        "    def forward(self, cosine, label):\n",
        "        # cosine: [batch, num_classes] - normalized cosine similarity\n",
        "        # label: [batch] - ground truth labels\n",
        "        \n",
        "        # Calculate sin(theta) for angular transformation\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        \n",
        "        # cos(theta + m) = cos(theta)*cos(m) - sin(theta)*sin(m)\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        \n",
        "        # Handle boundary conditions\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mmm)\n",
        "        \n",
        "        # Only apply margin to correct class\n",
        "        one_hot = torch.zeros(cosine.size()).type_as(cosine)\n",
        "        one_hot.scatter_(1, label.unsqueeze(1).long(), 1)\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        \n",
        "        # Apply scale factor\n",
        "        output *= self.scale\n",
        "        \n",
        "        loss = self.criterion(output, label)\n",
        "        return loss\n",
        "\n",
        "    def update(self, margin=0.2):\n",
        "        self.margin = margin\n",
        "        self.cos_m = math.cos(margin)\n",
        "        self.sin_m = math.sin(margin)\n",
        "        self.th = math.cos(math.pi - margin)\n",
        "        # Note: self.mm kept for compatibility with source code structure\n",
        "        self.mm = math.sin(math.pi - margin) * margin\n",
        "        self.mmm = 1.0 + math.cos(math.pi - margin)\n",
        "        self.m = self.margin  # For compatibility with source\n",
        "\n",
        "# Test the loss function\n",
        "loss_fn = ArcMarginLossDemo(scale=32.0, margin=0.2)\n",
        "print(\"ArcMarginLoss created successfully!\")\n",
        "print(f\"Scale: {loss_fn.scale}, Margin: {loss_fn.margin}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ArcFace Loss Visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create angle range\n",
        "theta = np.linspace(0, np.pi, 200)\n",
        "\n",
        "# Different margin values\n",
        "margins = [0.0, 0.2, 0.4, 0.5]\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Subplot 1: Angular transformation with different margins\n",
        "plt.subplot(1, 2, 1)\n",
        "for m in margins:\n",
        "    cos_theta = np.cos(theta)\n",
        "    cos_m = np.cos(m)\n",
        "    sin_m = np.sin(m)\n",
        "    sin_theta = np.sin(theta)\n",
        "    \n",
        "    # cos(theta + m)\n",
        "    cos_theta_m = cos_theta * cos_m - sin_theta * sin_m\n",
        "    \n",
        "    plt.plot(np.degrees(theta), cos_theta_m, label=f'm={m}', linewidth=2)\n",
        "\n",
        "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "plt.xlabel('Theta (degrees)')\n",
        "plt.ylabel('cos(theta + m)')\n",
        "plt.title('Angular Margin Effect')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Decision boundary visualization\n",
        "plt.subplot(1, 2, 2)\n",
        "angles = np.linspace(0, 2*np.pi, 100)\n",
        "for i, m in enumerate([0.0, 0.3]):\n",
        "    r1 = 1.0\n",
        "    x1 = r1 * np.cos(angles)\n",
        "    y1 = r1 * np.sin(angles)\n",
        "    plt.plot(x1, y1, '--' if m == 0 else '-', \n",
        "             label=f'Decision boundary (m={m})', alpha=0.7)\n",
        "\n",
        "# Draw class centers\n",
        "centers = [(0.7, 0.7), (-0.7, 0.7), (0, -0.9)]\n",
        "colors = ['red', 'blue', 'green']\n",
        "for (cx, cy), c in zip(centers, colors):\n",
        "    plt.scatter(cx, cy, c=c, s=100, marker='*', edgecolors='black', zorder=5)\n",
        "    \n",
        "plt.xlim(-1.5, 1.5)\n",
        "plt.ylim(-1.5, 1.5)\n",
        "plt.xlabel('Feature Dim 1')\n",
        "plt.ylabel('Feature Dim 2')\n",
        "plt.title('Feature Space with Angular Margin')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Parameters:\")\n",
        "print(\"- scale (s=32): Controls logits range, affects convergence speed\")\n",
        "print(\"- margin (m=0.2): Angular margin, increases inter-class distance\")\n",
        "print(\"- Training: margin increases from 0 to 0.2 (margin scheduling)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 2. CAM++ Architecture Deep Dive\n",
        "\n",
        "## 2.1 Architecture Overview\n",
        "\n",
        "CAM++ (Context-Aware Masking) is a state-of-the-art speaker embedding network that achieves excellent performance with relatively small model size (7.2M parameters).\n",
        "\n",
        "### Key Components\n",
        "\n",
        "```\n",
        "Input (B, T, 80)\n",
        "       |\n",
        "       v\n",
        "+------------------+\n",
        "|   FCM Module     |  <- ResNet-based 2D CNN for frequency modeling\n",
        "|  (Freq Conv)     |\n",
        "+------------------+\n",
        "       |\n",
        "       v\n",
        "+------------------+\n",
        "|   TDNN Layer     |  <- Initial temporal modeling\n",
        "+------------------+\n",
        "       |\n",
        "       v\n",
        "+------------------+\n",
        "| CAMDenseTDNN x3  |  <- Context-Aware Multi-scale Dense Blocks\n",
        "|   Block 1        |     12 layers, kernel=3, dilation=1\n",
        "|   Block 2        |     24 layers, kernel=3, dilation=2\n",
        "|   Block 3        |     16 layers, kernel=3, dilation=2\n",
        "+------------------+\n",
        "       |\n",
        "       v\n",
        "+------------------+\n",
        "|   Stats Pool     |  <- Mean + Std pooling\n",
        "+------------------+\n",
        "       |\n",
        "       v\n",
        "+------------------+\n",
        "|   Dense Layer    |  <- Final embedding (512-dim)\n",
        "+------------------+\n",
        "```\n",
        "\n",
        "## 2.2 FCM (Frequency Convolution Module)\n",
        "\n",
        "The FCM module uses 2D convolutions to model frequency patterns, similar to image processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# FCM Module Analysis\n",
        "# From speakerlab/models/campplus/DTDNN.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicResBlock(nn.Module):\n",
        "    \"\"\"Basic ResNet block for 2D convolution\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
        "                               stride=(stride, 1), padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes,\n",
        "                          kernel_size=1, stride=(stride, 1), bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)  # Residual connection\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class FCM(nn.Module):\n",
        "    \"\"\"\n",
        "    Frequency Convolution Module\n",
        "    Uses 2D CNN to process mel-spectrogram as image\n",
        "    \n",
        "    Args:\n",
        "        block: ResNet block type\n",
        "        num_blocks: Number of blocks per layer\n",
        "        m_channels: Channel dimension\n",
        "        feat_dim: Input feature dimension (mel bins)\n",
        "    \"\"\"\n",
        "    def __init__(self, block=BasicResBlock, num_blocks=[2, 2], \n",
        "                 m_channels=32, feat_dim=80):\n",
        "        super(FCM, self).__init__()\n",
        "        self.in_planes = m_channels\n",
        "        \n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(1, m_channels, kernel_size=3, \n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(m_channels)\n",
        "\n",
        "        # ResNet layers (downsample frequency dimension)\n",
        "        self.layer1 = self._make_layer(block, m_channels, num_blocks[0], stride=2)\n",
        "        self.layer2 = self._make_layer(block, m_channels, num_blocks[1], stride=2)\n",
        "\n",
        "        # Final convolution\n",
        "        self.conv2 = nn.Conv2d(m_channels, m_channels, kernel_size=3, \n",
        "                               stride=(2, 1), padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(m_channels)\n",
        "        \n",
        "        # Output channels: m_channels * (feat_dim // 8)\n",
        "        self.out_channels = m_channels * (feat_dim // 8)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, F, T) -> (B, 1, F, T)\n",
        "        x = x.unsqueeze(1)\n",
        "        \n",
        "        # Initial conv\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        \n",
        "        # ResNet blocks (downsample F by 4x)\n",
        "        out = self.layer1(out)  # F/2\n",
        "        out = self.layer2(out)  # F/4\n",
        "        \n",
        "        # Final conv (downsample F by 2x more)\n",
        "        out = F.relu(self.bn2(self.conv2(out)))  # F/8\n",
        "\n",
        "        # Reshape: (B, C, F/8, T) -> (B, C*F/8, T)\n",
        "        shape = out.shape\n",
        "        out = out.reshape(shape[0], shape[1]*shape[2], shape[3])\n",
        "        return out\n",
        "\n",
        "# Test FCM\n",
        "fcm = FCM(feat_dim=80, m_channels=32)\n",
        "dummy_input = torch.randn(2, 80, 200)  # (batch, freq, time)\n",
        "output = fcm(dummy_input)\n",
        "print(f\"FCM Input Shape: {dummy_input.shape}\")\n",
        "print(f\"FCM Output Shape: {output.shape}\")\n",
        "print(f\"FCM Output Channels: {fcm.out_channels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 CAM Layer (Context-Aware Masking)\n",
        "\n",
        "The core innovation of CAM++ is the **Context-Aware Masking** mechanism that combines:\n",
        "1. **Local features**: Standard 1D convolution\n",
        "2. **Global context**: Segment-level pooling + channel attention\n",
        "\n",
        "This allows the model to adaptively weight local and global information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# CAM Layer Analysis\n",
        "# From speakerlab/models/campplus/layers.py\n",
        "# Note: This is an educational demonstration of the CAMLayer structure.\n",
        "# The seg_pooling method below is part of the CAMLayer class.\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CAMLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Context-Aware Masking Layer\n",
        "    \n",
        "    Combines local convolution with global context attention.\n",
        "    \n",
        "    Key idea:\n",
        "    - Local: Standard convolution captures local patterns\n",
        "    - Global: Segment pooling + FC layers create attention mask\n",
        "    - Output: Local features * Global attention mask\n",
        "    \"\"\"\n",
        "    def __init__(self, bn_channels, out_channels, kernel_size,\n",
        "                 stride, padding, dilation, bias, reduction=2):\n",
        "        super(CAMLayer, self).__init__()\n",
        "        \n",
        "        # Local convolution branch\n",
        "        self.linear_local = nn.Conv1d(bn_channels, out_channels, kernel_size,\n",
        "                                      stride=stride, padding=padding,\n",
        "                                      dilation=dilation, bias=bias)\n",
        "        \n",
        "        # Global context branch (channel attention)\n",
        "        self.linear1 = nn.Conv1d(bn_channels, bn_channels // reduction, 1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.linear2 = nn.Conv1d(bn_channels // reduction, out_channels, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Local features\n",
        "        y = self.linear_local(x)\n",
        "        \n",
        "        # Global context: mean pooling + segment pooling\n",
        "        context = x.mean(-1, keepdim=True) + self.seg_pooling(x)\n",
        "        \n",
        "        # Attention mask through bottleneck\n",
        "        context = self.relu(self.linear1(context))\n",
        "        m = self.sigmoid(self.linear2(context))\n",
        "        \n",
        "        # Apply attention mask to local features\n",
        "        return y * m\n",
        "\n",
        "    def seg_pooling(self, x, seg_len=100, stype='avg'):\n",
        "        \"\"\"\n",
        "        Segment-level pooling for multi-scale context\n",
        "        \n",
        "        Divides input into segments and pools each segment,\n",
        "        then broadcasts back to original length.\n",
        "        \"\"\"\n",
        "        if stype == 'avg':\n",
        "            seg = F.avg_pool1d(x, kernel_size=seg_len, stride=seg_len, ceil_mode=True)\n",
        "        elif stype == 'max':\n",
        "            seg = F.max_pool1d(x, kernel_size=seg_len, stride=seg_len, ceil_mode=True)\n",
        "        else:\n",
        "            raise ValueError('Wrong segment pooling type.')\n",
        "        \n",
        "        # Broadcast pooled values back to original length\n",
        "        shape = seg.shape\n",
        "        seg = seg.unsqueeze(-1).expand(*shape, seg_len).reshape(*shape[:-1], -1)\n",
        "        seg = seg[..., :x.shape[-1]]\n",
        "        return seg\n",
        "\n",
        "# Visualize CAM mechanism\n",
        "print(\"CAM Layer combines:\")\n",
        "print(\"1. Local Conv: Captures local temporal patterns\")\n",
        "print(\"2. Global Context: Mean + Segment pooling\")\n",
        "print(\"3. Attention: Sigmoid gate on global context\")\n",
        "print(\"4. Output: Local * Attention (element-wise)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Complete CAM++ Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Complete CAM++ Model\n",
        "# From speakerlab/models/campplus/DTDNN.py\n",
        "#\n",
        "# IMPORTANT: This code demonstrates the architecture structure.\n",
        "# To run this, you need to import the helper classes from the project:\n",
        "#   from speakerlab.models.campplus.layers import (\n",
        "#       TDNNLayer, CAMDenseTDNNBlock, TransitLayer, \n",
        "#       StatsPool, DenseLayer, get_nonlinear\n",
        "#   )\n",
        "# The FCM class is defined in the previous code cell - ensure it is run first.\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import required components from speakerlab\n",
        "try:\n",
        "    from speakerlab.models.campplus.layers import (\n",
        "        TDNNLayer, CAMDenseTDNNBlock, TransitLayer,\n",
        "        StatsPool, DenseLayer, get_nonlinear\n",
        "    )\n",
        "    LAYERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Note: speakerlab.models.campplus.layers not available.\")\n",
        "    print(\"The CAMPPlus class below shows the architecture structure.\")\n",
        "    LAYERS_AVAILABLE = False\n",
        "\n",
        "class CAMPPlus(nn.Module):\n",
        "    \"\"\"\n",
        "    CAM++ Speaker Embedding Network\n",
        "    \n",
        "    Architecture:\n",
        "    1. FCM: 2D CNN for frequency modeling\n",
        "    2. TDNN: Initial temporal convolution\n",
        "    3. CAMDenseTDNN Blocks x3: Multi-scale context-aware blocks\n",
        "    4. Stats Pooling: Mean + Std aggregation\n",
        "    5. Dense: Final embedding projection\n",
        "    \"\"\"\n",
        "    def __init__(self, feat_dim=80, embedding_size=512, growth_rate=32,\n",
        "                 bn_size=4, init_channels=128, config_str='batchnorm-relu',\n",
        "                 memory_efficient=True):\n",
        "        super(CAMPPlus, self).__init__()\n",
        "\n",
        "        # FCM module for frequency processing\n",
        "        self.head = FCM(feat_dim=feat_dim)\n",
        "        channels = self.head.out_channels\n",
        "\n",
        "        # Build x-vector style network with CAM blocks\n",
        "        self.xvector = nn.Sequential(OrderedDict([\n",
        "            ('tdnn', TDNNLayer(channels, init_channels, 5, stride=2,\n",
        "                              dilation=1, padding=-1, config_str=config_str)),\n",
        "        ]))\n",
        "        channels = init_channels\n",
        "        \n",
        "        # Three CAMDenseTDNN blocks with different configurations\n",
        "        block_configs = [\n",
        "            (12, 3, 1),  # Block 1: 12 layers, kernel=3, dilation=1\n",
        "            (24, 3, 2),  # Block 2: 24 layers, kernel=3, dilation=2\n",
        "            (16, 3, 2),  # Block 3: 16 layers, kernel=3, dilation=2\n",
        "        ]\n",
        "        \n",
        "        for i, (num_layers, kernel_size, dilation) in enumerate(block_configs):\n",
        "            # Dense block with CAM layers\n",
        "            block = CAMDenseTDNNBlock(\n",
        "                num_layers=num_layers,\n",
        "                in_channels=channels,\n",
        "                out_channels=growth_rate,\n",
        "                bn_channels=bn_size * growth_rate,\n",
        "                kernel_size=kernel_size,\n",
        "                dilation=dilation,\n",
        "                config_str=config_str,\n",
        "                memory_efficient=memory_efficient\n",
        "            )\n",
        "            self.xvector.add_module(f'block{i+1}', block)\n",
        "            channels = channels + num_layers * growth_rate\n",
        "            \n",
        "            # Transition layer (channel reduction)\n",
        "            self.xvector.add_module(\n",
        "                f'transit{i+1}',\n",
        "                TransitLayer(channels, channels // 2, bias=False, config_str=config_str)\n",
        "            )\n",
        "            channels //= 2\n",
        "\n",
        "        # Output layers\n",
        "        self.xvector.add_module('out_nonlinear', get_nonlinear(config_str, channels))\n",
        "        self.xvector.add_module('stats', StatsPool())\n",
        "        self.xvector.add_module('dense', DenseLayer(channels * 2, embedding_size, \n",
        "                                                     config_str='batchnorm_'))\n",
        "        \n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, F) -> (B, F, T)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.head(x)\n",
        "        x = self.xvector(x)\n",
        "        return x\n",
        "\n",
        "# Model summary\n",
        "print(\"CAM++ Architecture Summary:\")\n",
        "print(\"- Input: (Batch, Time, 80) Fbank features\")\n",
        "print(\"- FCM: 2D CNN, outputs (B, 320, T)\")\n",
        "print(\"- TDNN: 1D Conv, outputs (B, 128, T/2)\")\n",
        "print(\"- Block1: 12 CAM layers, outputs (B, 512, T/2)\")\n",
        "print(\"- Transit1: Channel reduction to 256\")\n",
        "print(\"- Block2: 24 CAM layers, outputs (B, 1024, T/2)\")\n",
        "print(\"- Transit2: Channel reduction to 512\")\n",
        "print(\"- Block3: 16 CAM layers, outputs (B, 1024, T/2)\")\n",
        "print(\"- Transit3: Channel reduction to 512\")\n",
        "print(\"- Stats Pool: Mean + Std -> (B, 1024)\")\n",
        "print(\"- Dense: Final embedding (B, 512)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 3. Implementation\n",
        "\n",
        "## 3.1 Data Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Data Preprocessing Pipeline - Concept Demonstration\n",
        "# Based on speakerlab/process/processor.py\n",
        "#\n",
        "# NOTE: These are simplified demonstration classes showing the structure.\n",
        "# They return None as placeholders. For actual usage, import from speakerlab:\n",
        "#   from speakerlab.process.processor import WavReader, FBank\n",
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "class WavReaderDemo:\n",
        "    \"\"\"\n",
        "    Audio reader with speed perturbation and chunking\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_rate=16000, duration=3.0, speed_pertub=False):\n",
        "        self.duration = duration\n",
        "        self.sample_rate = sample_rate\n",
        "        self.speed_pertub = speed_pertub\n",
        "\n",
        "    def __call__(self, wav_path):\n",
        "        # In practice, use torchaudio.load(wav_path)\n",
        "        # wav, sr = torchaudio.load(wav_path)\n",
        "        \n",
        "        chunk_len = int(self.duration * self.sample_rate)\n",
        "        \n",
        "        # Speed perturbation (0.9x, 1.0x, 1.1x)\n",
        "        speed_idx = 0\n",
        "        if self.speed_pertub:\n",
        "            speed_idx = random.randint(0, 2)\n",
        "        \n",
        "        # Random chunk extraction\n",
        "        # if data_len >= chunk_len:\n",
        "        #     start = random.randint(0, data_len - chunk_len)\n",
        "        #     wav = wav[start:start + chunk_len]\n",
        "        # else:\n",
        "        #     wav = F.pad(wav, (0, chunk_len - data_len))\n",
        "        \n",
        "        return None, speed_idx\n",
        "\n",
        "class FBankDemo:\n",
        "    \"\"\"\n",
        "    Fbank feature extractor using Kaldi-compatible implementation\n",
        "    \"\"\"\n",
        "    def __init__(self, n_mels=80, sample_rate=16000, mean_nor=True):\n",
        "        self.n_mels = n_mels\n",
        "        self.sample_rate = sample_rate\n",
        "        self.mean_nor = mean_nor\n",
        "\n",
        "    def __call__(self, wav, dither=0):\n",
        "        # In practice, use torchaudio.compliance.kaldi.fbank\n",
        "        # feat = Kaldi.fbank(wav, num_mel_bins=self.n_mels, \n",
        "        #                    sample_frequency=self.sample_rate, dither=dither)\n",
        "        # if self.mean_nor:\n",
        "        #     feat = feat - feat.mean(0, keepdim=True)\n",
        "        return None\n",
        "\n",
        "print(\"Preprocessing Pipeline:\")\n",
        "print(\"1. Load audio -> Resample to 16kHz\")\n",
        "print(\"2. Speed perturbation (optional): 0.9x, 1.0x, 1.1x\")\n",
        "print(\"3. Random chunk extraction (3 seconds default)\")\n",
        "print(\"4. Fbank extraction: 80-dim, 25ms window, 10ms shift\")\n",
        "print(\"5. Mean normalization (optional)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Training Loop Example\n",
        "# Based on speakerlab/bin/train.py (see ../speakerlab/bin/train.py for full implementation)\n",
        "#\n",
        "# Note: In practice, the model is wrapped as nn.Sequential(embedding_model, classifier)\n",
        "# where classifier is a CosineClassifier that outputs logits for ArcFace loss.\n",
        "# The lr_scheduler and margin_scheduler are created using:\n",
        "#   - lr_scheduler: speakerlab.process.scheduler.WarmupCosineScheduler\n",
        "#   - margin_scheduler: speakerlab.process.scheduler.MarginScheduler\n",
        "\n",
        "def train_one_epoch(train_loader, model, criterion, optimizer, \n",
        "                    lr_scheduler, margin_scheduler, epoch):\n",
        "    \"\"\"\n",
        "    Training loop for one epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    \n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        # Update schedulers\n",
        "        iter_num = (epoch - 1) * len(train_loader) + i\n",
        "        lr_scheduler.step(iter_num)\n",
        "        margin_scheduler.step(iter_num)\n",
        "        \n",
        "        # Move to GPU\n",
        "        x = x.cuda(non_blocking=True)\n",
        "        y = y.cuda(non_blocking=True)\n",
        "        \n",
        "        # Forward pass: model = nn.Sequential(embedding_model, classifier)\n",
        "        # embedding_model outputs (B, 512) embeddings\n",
        "        # classifier (CosineClassifier) outputs (B, num_classes) logits\n",
        "        output = model(x)  # Returns logits for classification\n",
        "        loss = criterion(output, y)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Record metrics\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "print(\"Training Configuration (VoxCeleb):\")\n",
        "print(\"- Dataset: VoxCeleb2 dev set (5,994 speakers)\")\n",
        "print(\"  Download: https://www.robots.ox.ac.uk/~vgg/data/voxceleb/\")\n",
        "print(\"  Preparation: See egs/voxceleb/sv-cam++/local/prepare_data.sh\")\n",
        "print(\"- Epochs: 60\")\n",
        "print(\"- Batch size: 256\")\n",
        "print(\"- Learning rate: 0.1 -> 1e-4 (cosine decay)\")\n",
        "print(\"- Margin: 0.0 -> 0.2 (linear increase from epoch 15-25)\")\n",
        "print(\"- Optimizer: SGD with momentum 0.9\")\n",
        "print(\"- Weight decay: 1e-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 4. Model Export and Optimization\n",
        "\n",
        "## 4.1 ONNX Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ONNX Export\n",
        "# Implementation adapted from speakerlab's ONNX export logic\n",
        "# See ../speakerlab/bin/export_speaker_embedding_onnx.py for full script\n",
        "\n",
        "import torch\n",
        "\n",
        "def export_to_onnx(model, output_path, feat_dim=80):\n",
        "    \"\"\"\n",
        "    Export PyTorch model to ONNX format\n",
        "    \n",
        "    Args:\n",
        "        model: CAM++ model in eval mode\n",
        "        output_path: Path to save ONNX file\n",
        "        feat_dim: Input feature dimension\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Create dummy input: (batch=1, time=345, freq=80)\n",
        "    dummy_input = torch.randn(1, 345, feat_dim)\n",
        "    \n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        output_path,\n",
        "        export_params=True,\n",
        "        opset_version=11,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['feature'],\n",
        "        output_names=['embedding'],\n",
        "        dynamic_axes={\n",
        "            'feature': {0: 'batch_size', 1: 'frame_num'},\n",
        "            'embedding': {0: 'batch_size'}\n",
        "        }\n",
        "    )\n",
        "    print(f\"Model exported to {output_path}\")\n",
        "\n",
        "# Usage example:\n",
        "# export_to_onnx(model, \"campplus.onnx\")\n",
        "\n",
        "print(\"ONNX Export Notes:\")\n",
        "print(\"- Dynamic batch size and frame number\")\n",
        "print(\"- opset_version=11 for compatibility\")\n",
        "print(\"- Constant folding enabled for optimization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 INT8 Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# INT8 Quantization for Edge Deployment\n",
        "import numpy as np\n",
        "\n",
        "def quantize_onnx_model(onnx_path, output_path):\n",
        "    \"\"\"\n",
        "    Quantize ONNX model to INT8 using dynamic quantization\n",
        "    \n",
        "    Requires: onnxruntime, onnxruntime-extensions\n",
        "    \n",
        "    Args:\n",
        "        onnx_path: Path to FP32 ONNX model\n",
        "        output_path: Path to save INT8 model\n",
        "    \n",
        "    Note: For static quantization (better accuracy), you would need\n",
        "    calibration_data and use quantize_static() instead.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "        \n",
        "        # Dynamic quantization (no calibration data needed)\n",
        "        quantize_dynamic(\n",
        "            onnx_path,\n",
        "            output_path,\n",
        "            weight_type=QuantType.QInt8\n",
        "        )\n",
        "        print(f\"Quantized model saved to {output_path}\")\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"Please install: pip install onnxruntime\")\n",
        "\n",
        "# For static quantization with calibration:\n",
        "# from onnxruntime.quantization import quantize_static, CalibrationDataReader\n",
        "\n",
        "print(\"Quantization Options:\")\n",
        "print(\"1. Dynamic Quantization: No calibration, slightly lower accuracy\")\n",
        "print(\"2. Static Quantization: Requires calibration data, better accuracy\")\n",
        "print(\"3. QAT (Quantization-Aware Training): Best accuracy, requires retraining\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 5. Embedded Deployment Guide\n",
        "\n",
        "## 5.1 RK3568 Development Board Setup\n",
        "\n",
        "The RK3568 is an ARM-based SoC with NPU support, suitable for edge AI deployment.\n",
        "\n",
        "### Hardware Requirements\n",
        "- RK3568 Development Board\n",
        "- USB Microphone or Audio Input\n",
        "- Linux OS (Debian/Ubuntu based)\n",
        "\n",
        "### Software Requirements\n",
        "- RKNN Toolkit for NPU inference\n",
        "- ONNX Runtime for CPU inference\n",
        "- PortAudio for microphone input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# RK3568 Inference Script\n",
        "# Speaker verification on embedded device\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class SpeakerVerifier:\n",
        "    \"\"\"\n",
        "    Speaker verification system for RK3568\n",
        "    \n",
        "    Supports:\n",
        "    - ONNX Runtime (CPU)\n",
        "    - RKNN Runtime (NPU) - for better performance\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path, use_npu=False):\n",
        "        self.use_npu = use_npu\n",
        "        \n",
        "        if use_npu:\n",
        "            # RKNN for NPU acceleration\n",
        "            # from rknn.api import RKNN\n",
        "            # self.rknn = RKNN()\n",
        "            # self.rknn.load_rknn(model_path)\n",
        "            # self.rknn.init_runtime()\n",
        "            pass\n",
        "        else:\n",
        "            # ONNX Runtime for CPU\n",
        "            try:\n",
        "                import onnxruntime as ort\n",
        "            except ImportError:\n",
        "                raise RuntimeError(\n",
        "                    \"onnxruntime is required for CPU inference. \"\n",
        "                    \"Please install it with 'pip install onnxruntime'.\"\n",
        "                )\n",
        "            self.session = ort.InferenceSession(model_path)\n",
        "    \n",
        "    def extract_embedding(self, fbank_features):\n",
        "        \"\"\"\n",
        "        Extract speaker embedding from Fbank features\n",
        "        \n",
        "        Args:\n",
        "            fbank_features: numpy array (1, T, 80)\n",
        "        Returns:\n",
        "            embedding: numpy array (512,)\n",
        "        \"\"\"\n",
        "        if self.use_npu:\n",
        "            # outputs = self.rknn.inference(inputs=[fbank_features])\n",
        "            # return outputs[0]\n",
        "            pass\n",
        "        else:\n",
        "            outputs = self.session.run(\n",
        "                None, \n",
        "                {'feature': fbank_features.astype(np.float32)}\n",
        "            )\n",
        "            return outputs[0][0]\n",
        "    \n",
        "    def verify(self, emb1, emb2, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Verify if two embeddings are from same speaker\n",
        "        \n",
        "        Args:\n",
        "            emb1, emb2: Speaker embeddings\n",
        "            threshold: Cosine similarity threshold\n",
        "        Returns:\n",
        "            is_same: Boolean\n",
        "            score: Cosine similarity score\n",
        "        \"\"\"\n",
        "        # Normalize embeddings\n",
        "        emb1 = emb1 / np.linalg.norm(emb1)\n",
        "        emb2 = emb2 / np.linalg.norm(emb2)\n",
        "        \n",
        "        # Cosine similarity\n",
        "        score = np.dot(emb1, emb2)\n",
        "        \n",
        "        return score > threshold, score\n",
        "\n",
        "print(\"RK3568 Deployment Notes:\")\n",
        "print(\"- Use RKNN for NPU acceleration (3-5x faster)\")\n",
        "print(\"- Typical inference time: ~50ms (NPU), ~200ms (CPU)\")\n",
        "print(\"- Memory usage: ~100MB for model\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Real-time Microphone Recording and Verification\n",
        "import time\n",
        "\n",
        "class RealtimeSpeakerVerifier:\n",
        "    \"\"\"\n",
        "    Real-time speaker verification from microphone\n",
        "    \n",
        "    Requirements:\n",
        "    - pip install sounddevice numpy\n",
        "    - Working microphone\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path, sample_rate=16000, duration=3.0):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.duration = duration\n",
        "        self.chunk_samples = int(sample_rate * duration)\n",
        "        \n",
        "        # Initialize verifier\n",
        "        self.verifier = SpeakerVerifier(model_path)\n",
        "        \n",
        "        # Enrolled speaker embeddings\n",
        "        self.enrolled_speakers = {}\n",
        "    \n",
        "    def record_audio(self):\n",
        "        \"\"\"Record audio from microphone\"\"\"\n",
        "        try:\n",
        "            import sounddevice as sd\n",
        "            print(f\"Recording for {self.duration} seconds...\")\n",
        "            audio = sd.rec(self.chunk_samples, samplerate=self.sample_rate,\n",
        "                          channels=1, dtype='float32')\n",
        "            sd.wait()\n",
        "            return audio.flatten()\n",
        "        except ImportError:\n",
        "            print(\"Please install: pip install sounddevice\")\n",
        "            return None\n",
        "    \n",
        "    def extract_fbank(self, audio):\n",
        "        \"\"\"Extract Fbank features from audio\"\"\"\n",
        "        # PLACEHOLDER: Returns random data for demonstration only!\n",
        "        # For real usage, replace with actual Fbank extraction:\n",
        "        #   import torchaudio.compliance.kaldi as Kaldi\n",
        "        #   feat = Kaldi.fbank(wav, num_mel_bins=80, sample_frequency=16000)\n",
        "        import numpy as np\n",
        "        \n",
        "        # WARNING: Random features - replace with real Fbank extraction!\n",
        "        n_frames = len(audio) // 160  # 10ms shift\n",
        "        return np.random.randn(1, n_frames, 80).astype(np.float32)\n",
        "    \n",
        "    def enroll(self, speaker_name):\n",
        "        \"\"\"Enroll a new speaker\"\"\"\n",
        "        audio = self.record_audio()\n",
        "        if audio is None:\n",
        "            return False\n",
        "        \n",
        "        fbank = self.extract_fbank(audio)\n",
        "        embedding = self.verifier.extract_embedding(fbank)\n",
        "        \n",
        "        self.enrolled_speakers[speaker_name] = embedding\n",
        "        print(f\"Enrolled speaker: {speaker_name}\")\n",
        "        return True\n",
        "    \n",
        "    def verify_speaker(self, claimed_name, threshold=0.5):\n",
        "        \"\"\"Verify if speaker matches claimed identity\"\"\"\n",
        "        if claimed_name not in self.enrolled_speakers:\n",
        "            print(f\"Speaker {claimed_name} not enrolled\")\n",
        "            return False, 0.0\n",
        "        \n",
        "        audio = self.record_audio()\n",
        "        if audio is None:\n",
        "            return False, 0.0\n",
        "        \n",
        "        fbank = self.extract_fbank(audio)\n",
        "        embedding = self.verifier.extract_embedding(fbank)\n",
        "        \n",
        "        enrolled_emb = self.enrolled_speakers[claimed_name]\n",
        "        is_same, score = self.verifier.verify(enrolled_emb, embedding, threshold)\n",
        "        \n",
        "        print(f\"Verification result: {'PASS' if is_same else 'FAIL'}\")\n",
        "        print(f\"Similarity score: {score:.4f}\")\n",
        "        return is_same, score\n",
        "\n",
        "print(\"Usage Example:\")\n",
        "print(\"verifier = RealtimeSpeakerVerifier('campplus.onnx')\")\n",
        "print(\"verifier.enroll('Alice')  # Record and enroll\")\n",
        "print(\"verifier.verify_speaker('Alice')  # Verify identity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 6. Extensions\n",
        "\n",
        "## 6.1 Industrial Anomaly Detection\n",
        "\n",
        "The speaker verification framework can be adapted for industrial sound-based anomaly detection:\n",
        "\n",
        "### Application Scenarios\n",
        "1. **Equipment Monitoring**: Detect abnormal sounds from motors, pumps, fans\n",
        "2. **Predictive Maintenance**: Identify wear patterns before failure\n",
        "3. **Quality Control**: Detect defects in manufacturing through sound\n",
        "\n",
        "### Adaptation Strategy\n",
        "\n",
        "| Speaker Verification | Industrial Anomaly Detection |\n",
        "|---------------------|------------------------------|\n",
        "| Speaker embedding | Machine sound embedding |\n",
        "| Speaker identity | Normal/Abnormal state |\n",
        "| Cosine similarity | Anomaly score |\n",
        "| Enrollment | Baseline recording |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Industrial Anomaly Detection Adaptation\n",
        "import numpy as np\n",
        "\n",
        "class IndustrialAnomalyDetector:\n",
        "    \"\"\"\n",
        "    Adapt speaker verification for industrial anomaly detection\n",
        "    \n",
        "    Approach:\n",
        "    1. Collect normal operation sounds as baseline\n",
        "    2. Extract embeddings using CAM++ (or similar)\n",
        "    3. Compare new sounds against baseline\n",
        "    4. Flag anomalies when similarity drops\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path, threshold=0.7):\n",
        "        self.verifier = SpeakerVerifier(model_path)\n",
        "        self.threshold = threshold\n",
        "        self.baseline_embeddings = []\n",
        "        self.reference = None  # Initialize reference to None\n",
        "    \n",
        "    def train_baseline(self, normal_audio_files):\n",
        "        \"\"\"\n",
        "        Build baseline from normal operation recordings\n",
        "        \n",
        "        Args:\n",
        "            normal_audio_files: List of audio file paths\n",
        "        \"\"\"\n",
        "        for audio_file in normal_audio_files:\n",
        "            # Extract features and embedding\n",
        "            # fbank = extract_fbank(audio_file)\n",
        "            # emb = self.verifier.extract_embedding(fbank)\n",
        "            # self.baseline_embeddings.append(emb)\n",
        "            pass\n",
        "        \n",
        "        # Compute mean embedding as reference\n",
        "        if self.baseline_embeddings:\n",
        "            self.reference = np.mean(self.baseline_embeddings, axis=0)\n",
        "            self.reference = self.reference / np.linalg.norm(self.reference)\n",
        "    \n",
        "    def detect_anomaly(self, audio):\n",
        "        \"\"\"\n",
        "        Detect if audio contains anomaly\n",
        "        \n",
        "        Args:\n",
        "            audio: Audio samples or Fbank features\n",
        "        Returns:\n",
        "            is_anomaly: Boolean\n",
        "            anomaly_score: 1 - similarity (higher = more anomalous)\n",
        "        \"\"\"\n",
        "        # Check if baseline has been trained\n",
        "        if self.reference is None:\n",
        "            raise RuntimeError(\"Baseline not trained. Call train_baseline() first.\")\n",
        "        \n",
        "        # Extract embedding\n",
        "        # fbank = extract_fbank(audio)\n",
        "        # emb = self.verifier.extract_embedding(fbank)\n",
        "        # emb = emb / np.linalg.norm(emb)\n",
        "        \n",
        "        # Compare with baseline\n",
        "        # similarity = np.dot(emb, self.reference)\n",
        "        # anomaly_score = 1 - similarity\n",
        "        \n",
        "        # return anomaly_score > (1 - self.threshold), anomaly_score\n",
        "        pass\n",
        "\n",
        "print(\"Industrial Applications:\")\n",
        "print(\"1. Motor bearing fault detection\")\n",
        "print(\"2. Compressor anomaly monitoring\")\n",
        "print(\"3. Production line quality inspection\")\n",
        "print(\"4. HVAC system health monitoring\")\n",
        "print(\"\")\n",
        "print(\"Privacy & Ethics Considerations:\")\n",
        "print(\"- Ensure data collection consent\")\n",
        "print(\"- Secure storage of voice/sound data\")\n",
        "print(\"- Clear data retention policies\")\n",
        "print(\"- Transparency in monitoring systems\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "This tutorial covered:\n",
        "\n",
        "1. **Fundamentals**: Speaker verification concepts, Fbank features, ArcFace loss\n",
        "2. **CAM++ Architecture**: FCM module, CAM attention mechanism, DenseNet-style blocks\n",
        "3. **Implementation**: Data preprocessing, training loop, validation\n",
        "4. **Export & Optimization**: ONNX conversion, INT8 quantization\n",
        "5. **Embedded Deployment**: RK3568 setup, real-time inference\n",
        "6. **Extensions**: Industrial anomaly detection adaptation\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Train on VoxCeleb dataset using provided scripts\n",
        "- Experiment with different model configurations\n",
        "- Deploy to target hardware\n",
        "- Explore multi-modal extensions\n",
        "\n",
        "## References\n",
        "\n",
        "- [3D-Speaker GitHub](https://github.com/modelscope/3D-Speaker)\n",
        "- [CAM++ Paper](https://arxiv.org/abs/2303.00332)\n",
        "- [VoxCeleb Dataset](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)\n",
        "- [ONNX Runtime](https://onnxruntime.ai/)\n",
        "\n",
        "---\n",
        "\n",
        "**License**: Apache 2.0  \n",
        "**Acknowledgments**: 3D-Speaker Team, Alibaba DAMO Academy"
      ]
    }
  ]
}